Here’s a single copy-paste prompt you can drop into Replit’s AI to scaffold the whole project end-to-end. It’s opinionated, production-leaning, and enforces my coding style constraints.

---

Build an AI-powered service called “YT Shortsmith” that takes a YouTube video URL, automatically generates multiple short-form clips optimized for 9:16, categorizes them, scores them with an AI rubric to predict success, and stores both the rendered clips and their metadata. Provide a minimal web dashboard to submit URLs and review results.

Non-negotiable code style:

1. Never put comments on code.
2. Always put single if conditions inside brackets and with line break.

Stack:
• Node.js 20 + TypeScript
• Express for API
• PostgreSQL via Prisma
• Queue: BullMQ + Redis
• Video: ffmpeg via fluent-ffmpeg and ffmpeg-static
• YouTube download: yt-dlp invoked via child_process
• Transcription: OpenAI Whisper API
• Categorization + scoring: OpenAI responses (GPT-4o or better)
• Storage for rendered shorts: S3-compatible bucket (e.g., AWS S3 or Cloudflare R2) using @aws-sdk/client-s3
• Frontend: Next.js 14 app-router + TailwindCSS, served from the same repo (API under /api)
• Auth not required
• Dockerfile and Procfile for portability

Environment variables:
OPENAI_API_KEY
REDIS_URL
DATABASE_URL
S3_ENDPOINT
S3_REGION
S3_ACCESS_KEY_ID
S3_SECRET_ACCESS_KEY
S3_BUCKET
YT_DLP_PATH optional, default to yt-dlp in PATH

High-level flow:

1. User submits a YouTube URL on the dashboard.
2. API creates a video record, enqueues a job.
3. Worker downloads the video with yt-dlp, saves to a temp folder.
4. Extract audio and transcribe with Whisper API.
5. Segment detection combines:
   a) Voice activity and pause gaps from transcript timestamps
   b) Scene changes from ffmpeg select=gt(scene,0.3)
   c) Enforce target subclip duration 20–60 seconds, prefer 25–45 seconds, and ensure each subclip has a strong hook within first 3 seconds based on transcript lines
6. For each candidate segment:
   a) Convert to 9:16 with smart crop using ffmpeg’s cropdetect and scale; center important face regions using ffmpeg’s crop filters heuristics; if impossible, pillarbox
   b) Burn large, high-contrast subtitles from the transcript lines overlapping the segment
   c) Generate a one-line hook derived from first 3 seconds of speech, overlay as title safe-area text
7. Call OpenAI to categorize each subclip and score it with a rubric.
   Categories: Education, Motivation, Humor, Commentary, Tech, Lifestyle, News, Finance, Health, Sports, Gaming, Other
   Scores: hook_strength 0–10, retention_likelihood 0–10, clarity 0–10, shareability 0–10, overall 0–100
   Also return 3–7 tags and a one-sentence rationale
8. Upload mp4, thumbnail jpg, and srt to S3.
9. Store subclip metadata and AI outputs in PostgreSQL.
10. Dashboard lists all jobs, their subclips, categories, scores, links to files, filters, and sorting by score.

Database schema with Prisma:
model Video {
id           String   @id @default(cuid())
sourceUrl    String
title        String
durationSec  Int
status       String
createdAt    DateTime @default(now())
updatedAt    DateTime @updatedAt
transcript   Json?
clips        Clip[]
}

model Clip {
id             String   @id @default(cuid())
videoId        String
startSec       Int
endSec         Int
durationSec    Int
category       String
tags           String[]
scoreHook      Int
scoreRetention Int
scoreClarity   Int
scoreShare     Int
scoreOverall   Int
rationale      String
s3VideoKey     String
s3ThumbKey     String
s3SrtKey       String
createdAt      DateTime @default(now())
Video          Video    @relation(fields: [videoId], references: [id])
@@index([videoId])
}

API routes:
POST /api/submit
Body: { url: string }
Creates Video row, enqueues job, returns videoId.

GET /api/videos
Query: { q?, page?, pageSize?, sort? }
Returns list with pagination and sorting by scoreOverall.

GET /api/videos/:id
Returns the video record, derived stats, and clips.

Workers:
queue: video.process
job payload: { videoId }
Steps:
• Resolve metadata: title, duration
• Download best mp4 with yt-dlp
• Transcribe with Whisper API
• Detect candidate segments
• For each segment: render 1080x1920 mp4 with burnt captions and hook text, generate thumbnail via ffmpeg at first high-energy frame, write SRT
• Score + categorize via OpenAI with rubric prompt below
• Upload artifacts to S3
• Persist Clip rows
• Mark Video status

OpenAI scoring rubric prompt:
System:
You score short-form video clips for viral potential.

User:
Given the transcript segment, the first-3-second hook text, and context title, return a JSON object with:
category in [Education, Motivation, Humor, Commentary, Tech, Lifestyle, News, Finance, Health, Sports, Gaming, Other]
tags as an array of 3–7 short tags
scores as integers: hook_strength 0–10, retention_likelihood 0–10, clarity 0–10, shareability 0–10, overall 0–100
rationale as one sentence
Ensure the hook is compelling for a cold audience.

input:
title: {title}
hook: {hook}
transcript: {segmentText}

JSON only.

Segmentation details:
• Use transcript word timings to find pause gaps of 350–900 ms; break at longer pauses.
• Merge adjacent micro-segments to reach 20–60 seconds, prefer 25–45.
• Run ffmpeg scene detection to avoid mid-scene cuts; cut at scene change if within ±1.0s of a pause boundary.
• Discard segments with under 8 seconds of speech.
• Limit per-video to max 12 best candidates by quick heuristic score = words_per_second * 10 + pause_density * 5 + high-energy words count.

ffmpeg rendering:
• 1080x1920 output, 30fps, h264, crf 23, preset veryfast, aac 128k
• Smart crop: if source is wider than tall, scale width to 1080, compute crop for 1920 height; otherwise scale height to 1920 and crop width to 1080
• Burn SRT with large font and outline, title hook at top, safe margins 72px
• Normalize loudness to -14 LUFS

S3 layout:
s3://$S3_BUCKET/videos/{videoId}/source.mp4
s3://$S3_BUCKET/videos/{videoId}/clips/{clipId}/clip.mp4
s3://$S3_BUCKET/videos/{videoId}/clips/{clipId}/clip.srt
s3://$S3_BUCKET/videos/{videoId}/clips/{clipId}/thumb.jpg

Frontend pages:
• / : Submit URL form, table of videos with status and counts of clips, link to detail
• /videos/[id] : Grid of clips with player, category, tags, scores; filters by category and min score; download buttons

UI requirements:
• Tailwind
• Clean, dark UI
• Client calls to our API endpoints
• File links open S3 objects in new tabs

Installation tasks the AI must perform:
• Initialize Next.js + Express in a single repo: Next.js handles pages; API routes implemented in /app/api via route handlers; a dedicated worker script runs with ts-node for BullMQ consumer
• Add Prisma with PostgreSQL, generate client, run migrations for the schema
• Add Redis connection and BullMQ queue with a separate worker entrypoint src/worker.ts
• Add OpenAI SDK wrappers for transcription and scoring
• Add an ffmpeg service using ffmpeg-static path and fluent-ffmpeg
• Add S3 client helpers
• Add yt-dlp install step and detect binary path; fallback to pipx or a local binary download on first run
• Provide npm scripts:
dev
worker
migrate
db:push
lint
build
start
• Provide a Dockerfile that installs yt-dlp and ffmpeg, builds the app, and runs both web and worker via separate processes
• Provide a seed and a minimal e2e path test with Playwright that submits a known URL and verifies at least one Clip row is created with artifacts uploaded
• Ensure all code follows the two style rules above

Validation:
• Submit a real public YouTube URL from the UI
• Video row moves through statuses: queued → processing → completed
• At least 3 clips created unless video is too short
• Clips have categories, tags, and scores
• S3 has mp4, srt, and thumb for each clip
• Dashboard lists and sorts by overall score, and filters by category

Deliverables:
• Complete repository with working web app, API, worker, Prisma schema, Dockerfile, and README that explains env vars, how to run locally with Docker Compose for Postgres and Redis, and how to set S3 credentials in Replit Secrets.
• No code comments anywhere.
• Every single if statement uses braces with a line break on the next line style.

Start now. Implement everything described.
