Improve the YouTube → shorts pipeline to:

Split audio into the fewest chunks that respect OpenAI’s ~25MB upload limit. A 48MB audio should split into 2 chunks, not 11.

Skip the podcast intro by ignoring the first 2–3 minutes when selecting candidate clips. Make it configurable and default to 180s.

Make processing fault-tolerant. If some chunks or clip scorings fail, continue processing the rest. Retry transient errors and produce a final report of successes/failures instead of crashing the job.

Codebase Context

TypeScript/Node project with these relevant files:

queue-worker.ts – orchestrates processing jobs

queue.ts – queue helpers

youtube-source.ts – fetches video and metadata

ffmpeg.ts – media utils (ffprobe/ffmpeg wrappers)

transcript.ts – transcription and segmentation

srt.ts – subtitle utilities

redis.ts – persistence/locking

openai.ts – OpenAI client and current chunking logic for audio upload
Follow this style: never put comments inside code; always put single if conditions inside brackets and with a line break.

Tasks
A) Dynamic audio chunking by size (target ~24.5MB)

Implement size-aware chunk planning so the number of chunks = ceil(fileSizeBytes / targetBytes). Then derive a duration-based split plan from media duration to produce that many chunks. Use a safety margin of 24.5MB to stay under 25MB.

Edits:

In ffmpeg.ts: add a helper to get file size.

import fs from "fs";

export function getFileSizeBytes(path: string): number {
  const stat = fs.statSync(path);
  return stat.size;
}


In openai.ts: replace any fixed chunkDuration with dynamic calculation. Assume you already have getDurationSeconds(inputPath) from ffprobe.

const TARGET_MB = parseFloat(process.env.OPENAI_CHUNK_TARGET_MB || "24.5");
const TARGET_BYTES = Math.floor(TARGET_MB * 1024 * 1024);

export async function planAudioChunksBySize(inputPath: string, durationSec: number): Promise<{ start: number; duration: number }[]> {
  const sizeBytes = getFileSizeBytes(inputPath);
  let chunks = Math.ceil(sizeBytes / TARGET_BYTES);
  if (chunks < 1) {
    chunks = 1;
  }
  const base = Math.floor(durationSec / chunks);
  const rem = durationSec - base * chunks;
  const plan: { start: number; duration: number }[] = [];
  let cursor = 0;
  for (let i = 0; i < chunks; i++) {
    let d = base;
    if (i < rem) {
      d = d + 1;
    }
    plan.push({ start: cursor, duration: d });
    cursor = cursor + d;
  }
  return plan;
}


Use this plan wherever you previously computed chunkDuration. Ensure the actual exporting of chunk files respects -ss and -t per plan entry.

B) Skip podcast intro (default 180s) end-to-end

Introduce an intro skip that applies both to transcription and to clip selection so the AI stops picking teasers from the intro.

Add config:

INTRO_SKIP_SECONDS=180


In transcript.ts: when preparing the chunk plan, offset the start by skip = Math.min(INTRO_SKIP_SECONDS, durationSec). If the media is shorter than skip, process from 0. If using the plan above, shift the first chunk’s start to skip and recalculate total duration as durationSec - skip before planning.

const introSkip = parseInt(process.env.INTRO_SKIP_SECONDS || "180", 10);
let effectiveStart = 0;
if (durationSec > introSkip) {
  effectiveStart = introSkip;
}
const planned = await planAudioChunksBySize(inputPath, durationSec - effectiveStart);
const shifted = planned.map((p, idx) => {
  return { start: p.start + effectiveStart, duration: p.duration };
});


Use shifted for actual extraction and transcription. Keep absolute timestamps so downstream modules can filter reliably.

In any scoring or clip-picking module that traverses transcript segments, ignore segments that start before effectiveStart. If the selector currently uses raw segments, filter them:

const startCut = effectiveStart;
const usable = segments.filter(s => {
  return s.start >= startCut;
});


Apply the same filter where you generate candidate clips, so no clips begin before startCut.

C) Fault-tolerant pipeline

Ensure the system continues even if some steps fail.

In queue-worker.ts: never throw an unhandled error for per-chunk or per-clip work. Use Promise.allSettled for parallel work and collect results.

const results = await Promise.allSettled(tasks);
const ok = results.filter(r => {
  return r.status === "fulfilled";
});
const bad = results.filter(r => {
  return r.status === "rejected";
});


Persist both ok and bad with clear metadata in Redis so the UI can show partial success.

Add retries for transient OpenAI and network errors in openai.ts requests. Respect OPENAI_MAX_RETRIES and exponential backoff.

const MAX_RETRIES = parseInt(process.env.OPENAI_MAX_RETRIES || "2", 10);

export async function withRetries<T>(fn: () => Promise<T>): Promise<T> {
  let attempt = 0;
  let delay = 1000;
  for (;;) {
    try {
      const res = await fn();
      return res;
    } catch (err: any) {
      attempt = attempt + 1;
      const code = err?.status || err?.code || 0;
      const transient = code === 429 || code === 408 || code === 500 || code === 502 || code === 503 || code === 504;
      if (attempt > MAX_RETRIES) {
        throw err;
      }
      if (!transient) {
        throw err;
      }
      await new Promise(r => {
        setTimeout(r, delay);
      });
      delay = delay * 2;
    }
  }
}


Wrap OpenAI transcription and scoring calls with withRetries.

Wherever a chunk fails to transcribe or score, mark it failed and continue. For arrays of candidate clips, also use Promise.allSettled so one bad clip does not abort the batch.

Ensure queue.ts and queue-worker.ts catch top-level job errors, store a final status like:

completedCount

failedCount

failedItems with identifiers and reasons

introSkippedSeconds used

chunkPlan used

Return this summary in the API response for visibility in the UI logs.

D) Environment and config

Add or update:

OPENAI_CHUNK_TARGET_MB=24.5
OPENAI_MAX_RETRIES=2
INTRO_SKIP_SECONDS=180

E) Acceptance Criteria

Given a 48MB audio file, the computed plan produces exactly 2 chunks and each upload is ≤ 24.5MB.

If the video is ≥ 180s, transcription and clip selection both start at timestamp ≥ 180. No clips start before 180s.

When one chunk’s transcription fails with a transient error, it is retried up to the configured limit. If it still fails, the job completes with partial success and an error report; other chunks are processed and scored.

The UI logs show a per-job summary with counts and any failures.

Coding style: no comments inside code; every single-line if uses braces on their own lines.

F) Manual Test Plan

Use a known large podcast episode with audio ≈ 48–60MB. Verify chunk plan count = ceil(size / 24.5MB).

Set INTRO_SKIP_SECONDS=180. Verify that first transcript segment considered by scorers starts ≥ 180s and that generated clips exclude the intro.

Temporarily force one OpenAI call to throw 500. Verify retries then partial success with the rest of the chunks.

Reduce OPENAI_CHUNK_TARGET_MB to 8 and confirm the number of chunks scales up accordingly and still completes.

Short video (< 180s): verify no skip is applied and the whole video is eligible.

G) Notes

Keep absolute timestamps in transcripts; apply filtering in selection/scoring.

Make sure ffmpeg.ts extraction uses the shifted start and duration plan as provided by planAudioChunksBySize.

Ensure all API routes return usable JSON logs so the frontend page.tsx shows meaningful progress.