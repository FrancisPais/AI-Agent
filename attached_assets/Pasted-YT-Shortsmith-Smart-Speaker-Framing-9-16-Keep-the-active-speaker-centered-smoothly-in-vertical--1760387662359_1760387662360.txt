YT Shortsmith — Smart Speaker Framing (9:16)

Keep the active speaker centered smoothly in vertical clips, with zero regressions to the existing pipeline.

1) Scope & Deliverables

Add a smart framing stage that centers the active speaker in 9:16.

Smooth, natural camera pan, speed-limited, with easing on speaker changes.

Zero regressions: falls back to the current crop when needed.

Feature-flagged and safe to roll out gradually.

2) Install & Stack

Add dependencies:

Face detection: @vladmandic/face-api or @mediapipe/face_detection (Node TFJS)

Optional diarization: microservice later; start with transcript-based active speaker heuristic

Utility: ml-matrix or simple moving average for smoothing

Example install:

pnpm add @vladmandic/face-api @tensorflow/tfjs-node

3) Environment Variables

Add these to .env:

FEATURE_SMART_FRAMING=true
FRAMING_SAMPLE_FPS=3
FRAMING_SAFETY_MARGIN=0.12
FRAMING_MAX_PAN_PX_PER_S=600
FRAMING_EASE_MS=250
FRAMING_MIN_SPEAKER_HOLD_MS=600
FRAMING_CENTER_BIAS_Y=0.08
FRAMING_BOTTOM_SAFE_PCT=0.15
FRAMING_TOP_SAFE_PCT=0.10

4) Data Model (Prisma)

Non-breaking additions:

model Clip {
  id            String   @id @default(cuid())
  videoId       String
  startSec      Float
  endSec        Float
  ...
  smartFramed   Boolean  @default(false)
  cropMapJson   Json?
}

5) High-Level Flow

For each segment, build an active speaker timeline from transcript timestamps.

Detect and track faces at low fps.

Map active speaker windows to the best face track.

Generate keyframes for a 9:16 crop box that keeps the face centered with headroom and UI-safe zones.

Smooth keyframes, limit pan speed, add easing on speaker changes.

Render with ffmpeg using a single filter_complex piecewise crop expression.

Fallback to current crop if anything fails.

6) Service: framingService.ts

Create src/services/framingService.ts:

type TranscriptWord = { t: number; end: number; text: string; speaker?: string }
type SpeakerWindow = { start: number; end: number; speakerId: string }
type FaceBox = { t: number; x: number; y: number; w: number; h: number }
type FaceTrack = { id: string; boxes: FaceBox[] }
type CropKF = { t: number; x: number; y: number; w: number; h: number }
type ComputeInput = { videoPath: string; baseW: number; baseH: number; segStart: number; segEnd: number; transcript: TranscriptWord[] }
type Constraints = { margin: number; maxPan: number; easeMs: number; centerBiasY: number; safeTop: number; safeBottom: number }

export async function computeCropMap(input: ComputeInput, c: Constraints): Promise<CropKF[] | null> {
  const speaker = buildSpeakerTimeline(input.transcript, input.segStart, input.segEnd)
  const faces = await detectAndTrackFaces(input.videoPath, input.segStart, input.segEnd)
  const mapping = mapSpeakersToTracks(speaker, faces)
  const raw = buildKeyframes(mapping, faces, input.baseW, input.baseH, c)
  const smoothed = smoothAndConstrain(raw, input.baseW, input.baseH, input.segStart, input.segEnd, c)
  if (!smoothed.length) {
    return null
  }
  return smoothed
}

function buildSpeakerTimeline(words: TranscriptWord[], segStart: number, segEnd: number): SpeakerWindow[] {
  const win: SpeakerWindow[] = []
  let cur = null as SpeakerWindow | null
  for (const w of words) {
    if (w.t < segStart) {
      continue
    }
    if (w.t > segEnd) {
      break
    }
    const sp = w.speaker ?? 'spk'
    if (!cur) {
      cur = { start: Math.max(w.t, segStart), end: Math.min(w.end, segEnd), speakerId: sp }
      continue
    }
    if (cur.speakerId === sp) {
      cur.end = Math.min(w.end, segEnd)
    } else {
      if (cur.end - cur.start > 0) {
        win.push(cur)
      }
      cur = { start: Math.max(w.t, segStart), end: Math.min(w.end, segEnd), speakerId: sp }
    }
  }
  if (cur && cur.end - cur.start > 0) {
    win.push(cur)
  }
  return mergeShortWindows(win)
}

function mergeShortWindows(w: SpeakerWindow[]): SpeakerWindow[] {
  if (w.length === 0) {
    return w
  }
  const out: SpeakerWindow[] = []
  let cur = w[0]
  for (let i = 1; i < w.length; i++) {
    const nxt = w[i]
    if (nxt.start - cur.end < 0.6 && nxt.speakerId === cur.speakerId) {
      cur.end = nxt.end
    } else {
      out.push(cur)
      cur = nxt
    }
  }
  out.push(cur)
  return out.filter(s => s.end - s.start >= 0.6)
}

async function detectAndTrackFaces(videoPath: string, segStart: number, segEnd: number): Promise<FaceTrack[]> {
  return []
}

function mapSpeakersToTracks(s: SpeakerWindow[], tracks: FaceTrack[]): Array<{ start: number; end: number; trackId: string }> {
  const out: Array<{ start: number; end: number; trackId: string }> = []
  for (const sw of s) {
    let best = null as { id: string; overlap: number } | null
    for (const t of tracks) {
      const ov = overlapSeconds(sw.start, sw.end, t.boxes[0]?.t ?? 0, t.boxes[t.boxes.length - 1]?.t ?? 0)
      if (!best || ov > best.overlap) {
        best = { id: t.id, overlap: ov }
      }
    }
    if (best && best.overlap > 0) {
      out.push({ start: sw.start, end: sw.end, trackId: best.id })
    }
  }
  return out
}

function overlapSeconds(a: number, b: number, c: number, d: number): number {
  const x = Math.max(a, c)
  const y = Math.min(b, d)
  if (y <= x) {
    return 0
  }
  return y - x
}

function buildKeyframes(mapping: Array<{ start: number; end: number; trackId: string }>, tracks: FaceTrack[], baseW: number, baseH: number, c: Constraints): CropKF[] {
  const out: CropKF[] = []
  const targetW = Math.floor(baseH * 9 / 16)
  const targetH = baseH
  for (const m of mapping) {
    const tr = tracks.find(t => t.id === m.trackId)
    if (!tr) {
      continue
    }
    for (const b of tr.boxes) {
      if (b.t < m.start) {
        continue
      }
      if (b.t > m.end) {
        break
      }
      const cx = b.x + b.w / 2
      const cy = b.y + b.h * (0.5 - c.centerBiasY)
      let x = Math.round(cx - targetW / 2)
      let y = Math.round(cy - targetH / 2)
      x = Math.max(0, Math.min(x, baseW - targetW))
      const safeTop = Math.round(targetH * c.safeTop)
      const safeBottom = Math.round(targetH * c.safeBottom)
      y = Math.max(-safeTop, Math.min(y, baseH - targetH + safeBottom))
      out.push({ t: b.t, x, y, w: targetW, h: targetH })
    }
  }
  return dedupeByTime(out, 0.1)
}

function dedupeByTime(kf: CropKF[], minDelta: number): CropKF[] {
  if (kf.length === 0) {
    return kf
  }
  const out: CropKF[] = [kf[0]]
  for (let i = 1; i < kf.length; i++) {
    if (kf[i].t - out[out.length - 1].t >= minDelta) {
      out.push(kf[i])
    }
  }
  return out
}

function smoothAndConstrain(kf: CropKF[], baseW: number, baseH: number, segStart: number, segEnd: number, c: Constraints): CropKF[] {
  if (kf.length === 0) {
    return kf
  }
  const win = 3
  const sm: CropKF[] = []
  for (let i = 0; i < kf.length; i++) {
    let sx = 0
    let sy = 0
    let n = 0
    for (let j = Math.max(0, i - win); j <= Math.min(kf.length - 1, i + win); j++) {
      sx += kf[j].x
      sy += kf[j].y
      n += 1
    }
    const x = Math.round(sx / n)
    const y = Math.round(sy / n)
    sm.push({ t: kf[i].t, x, y, w: kf[i].w, h: kf[i].h })
  }
  const out: CropKF[] = [sm[0]]
  for (let i = 1; i < sm.length; i++) {
    const dt = Math.max(0.001, sm[i].t - sm[i - 1].t)
    const dx = sm[i].x - out[out.length - 1].x
    const dy = sm[i].y - out[out.length - 1].y
    const maxStep = Math.round(c.maxPan * dt)
    const nx = stepLimit(out[out.length - 1].x, sm[i].x, maxStep)
    const ny = stepLimit(out[out.length - 1].y, sm[i].y, maxStep)
    out.push({ t: sm[i].t, x: nx, y: ny, w: sm[i].w, h: sm[i].h })
  }
  return easeSpeakerEdges(out, segStart, segEnd, c.easeMs / 1000)
}

function stepLimit(a: number, b: number, m: number): number {
  if (Math.abs(b - a) <= m) {
    return b
  }
  if (b > a) {
    return a + m
  }
  return a - m
}

function easeSpeakerEdges(kf: CropKF[], segStart: number, segEnd: number, easeS: number): CropKF[] {
  if (kf.length < 2) {
    return kf
  }
  const out: CropKF[] = [kf[0]]
  for (let i = 1; i < kf.length; i++) {
    const t = kf[i].t
    const p = out[out.length - 1]
    const alpha = Math.min(1, Math.max(0, (t - segStart) / easeS, (segEnd - t) / easeS))
    const x = Math.round(p.x + (kf[i].x - p.x) * alpha)
    const y = Math.round(p.y + (kf[i].y - p.y) * alpha)
    out.push({ t, x, y, w: kf[i].w, h: kf[i].h })
  }
  return out
}

export function buildPiecewiseExpr(kf: CropKF[], key: 'x' | 'y'): string {
  if (kf.length === 0) {
    return '0'
  }
  const parts: string[] = []
  for (let i = 0; i < kf.length - 1; i++) {
    const a = kf[i]
    const b = kf[i + 1]
    const ta = a.t
    const tb = b.t
    const va = key === 'x' ? a.x : a.y
    const vb = key === 'x' ? b.x : b.y
    const slope = (vb - va) / Math.max(0.001, tb - ta)
    const expr = `between(t,${ta.toFixed(3)},${tb.toFixed(3)})*(${va.toFixed(0)}+(${slope.toFixed(6)})*(t-${ta.toFixed(3)}))`
    parts.push(expr)
  }
  const last = key === 'x' ? kf[kf.length - 1].x : kf[kf.length - 1].y
  parts.push(`gte(t,${kf[kf.length - 1].t.toFixed(3)})*${last.toFixed(0)}`)
  return parts.join('+')
}

7) Worker Integration

In your clip generation worker, before ffmpeg:

import { computeCropMap, buildPiecewiseExpr } from '@/services/framingService'

export async function generateClipSmartFramed(seg) {
  if (process.env.FEATURE_SMART_FRAMING !== 'true') {
    return null
  }
  const c = {
    margin: Number(process.env.FRAMING_SAFETY_MARGIN || 0.12),
    maxPan: Number(process.env.FRAMING_MAX_PAN_PX_PER_S || 600),
    easeMs: Number(process.env.FRAMING_EASE_MS || 250),
    centerBiasY: Number(process.env.FRAMING_CENTER_BIAS_Y || 0.08),
    safeTop: Number(process.env.FRAMING_TOP_SAFE_PCT || 0.1),
    safeBottom: Number(process.env.FRAMING_BOTTOM_SAFE_PCT || 0.15)
  }
  const cropMap = await computeCropMap({
    videoPath: seg.inputPath,
    baseW: seg.baseW,
    baseH: seg.baseH,
    segStart: seg.start,
    segEnd: seg.end,
    transcript: seg.transcript
  }, c)
  if (!cropMap) {
    return null
  }
  return cropMap
}


Persist on the Clip:

cropMapJson = cropMap

smartFramed = true

8) Rendering With ffmpeg (single pass)

When cropMap exists:

export async function renderWithSmartCrop(seg, cropMap) {
  const exprX = buildPiecewiseExpr(cropMap, 'x')
  const exprY = buildPiecewiseExpr(cropMap, 'y')
  const cropW = Math.floor(seg.baseH * 9 / 16)
  const cropH = seg.baseH
  const vf = `crop=${cropW}:${cropH}:${exprX}:${exprY},scale=1080:1920,format=yuv420p`
  const args = [
    '-ss', String(seg.start),
    '-to', String(seg.end),
    '-i', seg.inputPath,
    '-r', '30',
    '-an',
    '-vf', vf,
    '-c:v', 'libx264',
    '-preset', 'veryfast',
    '-crf', '18',
    '-pix_fmt', 'yuv420p',
    '-movflags', '+faststart',
    seg.tmpVideoPath
  ]
  const ok = await runFfmpeg(args)
  if (!ok) {
    return null
  }
  return seg.tmpVideoPath
}


If renderWithSmartCrop returns null, immediately fall back to your existing render path.

9) Safe Zones & Headroom Rules

Keep top UI area ≈ 10% free, bottom subtitles ≈ 15% free.

Headroom: center the face slightly below vertical center by FRAMING_CENTER_BIAS_Y (e.g., 0.08).

Ignore speaker flips shorter than FRAMING_MIN_SPEAKER_HOLD_MS (600 ms) inside mergeShortWindows.

10) Fallbacks

No face detected: return null from computeCropMap, keep current behavior.

Low-res or heavy motion: lower FRAMING_SAMPLE_FPS to 2, widen smoothing window if needed.

Two faces arguing quickly: maintain a short mid crop for 0.3–0.5 s by merging windows, then slide.

11) Rollout Plan

Stage 1: Enable on 10% of jobs via env at deploy time.

Stage 2: 50% after metrics are green.

Stage 3: 100%, keep feature flag for rapid disable.

12) Metrics & QA

Track per clip:

Face-center hit rate: % frames where the speaker face center lies inside the central 40% circle

Jitter: mean delta of crop center < 2 px/frame

Fallback rate: < 10%

Platform A/B: 3-second watch rate, completion rate, CTR

Manual QA checklist:

Hard speaker switches feel eased, not jumpy

Subtitles never overlap the mouth

No black borders or out-of-bounds crops

Works on 720p, 1080p, 4K sources

13) Test Hooks

Add a worker arg or env to force smart framing on a known test video segment and export the generated cropMapJson alongside the clip for inspection.

14) Future Upgrade (optional, later)

Replace heuristic with real diarization sidecar and stable face re-ID

Add multi-layout presets (split-screen, remote, in-studio)

Auto-thumbnail with centered speaker

Hand-off Summary

Create framingService.ts with the stubs above and wire it into the worker before ffmpeg.

Store cropMapJson on Clip, set smartFramed.

Build the crop= piecewise expressions with between(t,...).

Respect env flags and fall back immediately when null.

Ship progressively and measure.